{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d879c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "import numpy\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout, Attention\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a3d80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.set_visible_devices(gpus[:1], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee26d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"Available GPUs: {len(gpus)}\")\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"GPU {i}: {gpu}\")\n",
    "else:\n",
    "    print(\"No GPUs available.\")    \n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d185377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.w1 = Dense(units)\n",
    "        self.w2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "        \n",
    "    def __call__(self, values, query):\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.w1(values) + self.w2(hidden_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "            \n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "            \n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# 파라미터 튜닝 결과 모든 모델에서 1개가 최적이었으므로 노드 수만 조절\n",
    "def DDI_model(unit):\n",
    "    input_A = layers.Input(shape=(train_A.shape[1], 700), name = 'inputA')\n",
    "    input_B = layers.Input(shape=(train_A.shape[1], 700), name = 'inputB')\n",
    "\n",
    "    lstm_A, forward_h_A, forward_c_A, backward_h_A, backward_c_A = Bidirectional(LSTM(64, return_sequences=True, return_state = True), name = \"Bi_A\")(input_A)\n",
    "    lstm_B, forward_h_B, forward_c_B, backward_h_B, backward_c_B = Bidirectional(LSTM(64, return_sequences=True, return_state = True), name = \"Bi_B\")(input_B)\n",
    "\n",
    "    state_h_A = Concatenate()([forward_h_A, backward_h_A])\n",
    "    state_h_B = Concatenate()([forward_h_B, backward_h_B])\n",
    "\n",
    "    attention_A = Attention(80)\n",
    "    context_vector_A, attention_weight_A = attention_A(lstm_A, state_h_A)\n",
    "\n",
    "    attention_B= Attention(80)\n",
    "    context_vector_B, attention_weight_B = attention_B(lstm_B, state_h_B)\n",
    "\n",
    "    concat_AB = Concatenate()([context_vector_A, context_vector_B])\n",
    "\n",
    "    fc = Dense(units = unit, activation = 'relu', kernel_initializer = 'he_normal')(concat_AB)\n",
    "\n",
    "    output = Dense(units = num_class, activation = 'softmax')(fc)\n",
    "\n",
    "    model = Model(inputs = [input_A,input_B], outputs = output)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "# 데이터 로드\n",
    "train_set = np.load('../Generated_data/train_set.npz')\n",
    "val_set = np.load('../Generated_data/val_set.npz')\n",
    "\n",
    "train_A = train_set['train_A']\n",
    "train_B = train_set['train_B']\n",
    "\n",
    "val_A = val_set['val_A']\n",
    "val_B = val_set['val_B']\n",
    "one_hot_train = pd.read_csv('../Generated_data/one_hot_train.csv', index_col = 0)\n",
    "one_hot_val = pd.read_csv('../Generated_data/one_hot_val.csv', index_col = 0)\n",
    "\n",
    "num_class = len(one_hot_train.columns)\n",
    "del train_set, val_set\n",
    "\n",
    "es = EarlyStopping(patience = 10, restore_best_weights = True)\n",
    "mc = ModelCheckpoint(\"Model_save/model_training.h5\", save_best_only = True)\n",
    "\n",
    "model = DDI_model(128)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "model.fit({'inputA':train_A, 'inputB':train_B}, one_hot_train, \n",
    "          epochs = 100, \n",
    "          batch_size = 32, \n",
    "          verbose = 2, \n",
    "          validation_data=({'inputA':val_A, 'inputB':val_B}, one_hot_val),\n",
    "          callbacks = [es, mc])\n",
    "\n",
    "model.save('Model_save/model_final.h5', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f033e54b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddi4",
   "language": "python",
   "name": "ddi4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
